{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepComp_Comparison_LSTM_Wind.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "12SpbdQs6BV-XVLpMWbtHcbruJ-s40DxV",
      "authorship_tag": "ABX9TyOWe/Ldu5InhQJSOw1dGpM0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDQqKc8C-Ofy"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jz07bAo6t5G"
      },
      "source": [
        "data_train_csv1 = pd.read_csv('https://raw.githubusercontent.com/Jaeik-Jeong/DeepComp/main/Wind_Wallonie_Elia_16.csv', index_col=0)\r\n",
        "data_train_csv2 = pd.read_csv('https://raw.githubusercontent.com/Jaeik-Jeong/DeepComp/main/Wind_Wallonie_Elia_17.csv', index_col=0)\r\n",
        "data_train_csv  = pd.concat([data_train_csv1, data_train_csv2])\r\n",
        "data_val_csv    = pd.read_csv('https://raw.githubusercontent.com/Jaeik-Jeong/DeepComp/main/Wind_Wallonie_Elia_18.csv', index_col=0)\r\n",
        "data_test_csv   = pd.read_csv('https://raw.githubusercontent.com/Jaeik-Jeong/DeepComp/main/Wind_Wallonie_Elia_19.csv', index_col=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUtd8sOT-UoQ"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "unit = 4 #unit: 15 minute\n",
        "\n",
        "RE_Capacity1 = max(data_train_csv['Power(MW)'])\n",
        "RE_Capacity2 = max(data_val_csv['Power(MW)'])\n",
        "RE_Capacity3 = max(data_test_csv['Power(MW)'])\n",
        "\n",
        "size_train0 = int(len(data_train_csv)/unit)\n",
        "size_val0   = int(len(data_val_csv)/unit)\n",
        "size_test0  = int(len(data_test_csv)/unit)\n",
        "\n",
        "data_train0 = []\n",
        "data_train  = []\n",
        "for i in range(size_train0):\n",
        "    data_train0 += [round(pd.Series.mean(data_train_csv['Power(MW)'][i*unit:(i+1)*unit])/RE_Capacity1, 3)]\n",
        "    data_train  += [data_train0[i]] if data_train0[i] > 0 else []\n",
        "\n",
        "data_val0 = []\n",
        "data_val  = []\n",
        "for i in range(size_val0):\n",
        "    data_val0 += [round(pd.Series.mean(data_val_csv['Power(MW)'][i*unit:(i+1)*unit])/RE_Capacity2, 3)]\n",
        "    data_val  += [data_val0[i]] if data_val0[i] > 0 else []\n",
        "\n",
        "data_test0 = []\n",
        "data_test  = []\n",
        "for i in range(size_test0):\n",
        "    data_test0 += [round(pd.Series.mean(data_test_csv['Power(MW)'][i*unit:(i+1)*unit])/RE_Capacity3, 3)]\n",
        "    data_test  += [data_test0[i]] if data_test0[i] > 0 else []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ws0q-rxs-V44"
      },
      "source": [
        "# LSTM\n",
        "\n",
        "n_layers       = 1\n",
        "in_size        = 1\n",
        "hidden_size    = 16\n",
        "out_size       = 1\n",
        "batch_size     = 128\n",
        "learning_rate  = 0.001\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.fc_in  = nn.Linear(in_size, hidden_size)\n",
        "        self.rnn    = nn.LSTM(hidden_size, hidden_size, n_layers, batch_first=True)\n",
        "        self.fc_out = nn.Linear(hidden_size, out_size)\n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        x = F.relu(self.fc_in(x))\n",
        "        x = x.view(1, -1, hidden_size)\n",
        "        x, hidden = self.rnn(x, hidden)\n",
        "        out = self.fc_out(x)\n",
        "        out = F.relu(out.view(-1, out_size))\n",
        "        return out, hidden\n",
        "        \n",
        "def train_net(model, batch, optimizer):\n",
        "    x, h, y = batch[0], batch[1], batch[2]\n",
        "    loss = F.mse_loss(model.forward(x, h)[0], y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5rVzBsK-Wyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5dacf4-ce3d-40d7-a975-49c7092c5bc6"
      },
      "source": [
        "# Training LSTM\n",
        "\n",
        "total_epoch    = 200\n",
        "print_interval = 10\n",
        "\n",
        "model = LSTM()\n",
        "size_train = len(data_train)\n",
        "size_val = len(data_val)\n",
        "size_test = len(data_test)\n",
        "\n",
        "train_input = np.zeros((size_train-1, 1))\n",
        "train_output = np.zeros((size_train-1, 1))\n",
        "for i in range(size_train-1):\n",
        "    train_input[i,:] = data_train[i]\n",
        "    train_output[i,:] = data_train[i+1]\n",
        "\n",
        "val_input = np.zeros((size_val-1, 1))\n",
        "val_output = np.zeros((size_val-1, 1))\n",
        "for i in range(size_val-1):\n",
        "    val_input[i,:] = data_val[i]\n",
        "    val_output[i,:] = data_val[i+1]\n",
        "\n",
        "test_input = np.zeros((size_test-1, 1))\n",
        "test_output = np.zeros((size_test-1, 1))\n",
        "for i in range(size_test-1):\n",
        "    test_input[i,:] = data_test[i]\n",
        "    test_output[i,:] = data_test[i+1]\n",
        "\n",
        "total_batch = int((size_train-1)/batch_size) + 1\n",
        "pred_train, pred_val, pred_test = [], [], [] # Predicted Value\n",
        "mape_train, mape_val, mape_test = [], [], [] # Mean Absolute Percentage Error\n",
        "\n",
        "hidden = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for epoch in range(total_epoch):\n",
        "    for i in range(total_batch):\n",
        "        batch_x = torch.tensor(train_input[batch_size*i:batch_size*(i+1),:] ,dtype=torch.float)\n",
        "        batch_y = torch.tensor(train_output[batch_size*i:batch_size*(i+1),:] ,dtype=torch.float)\n",
        "        batch = [batch_x, hidden, batch_y]\n",
        "        train_net(model, batch, optimizer)\n",
        "        _, hidden = model.forward(batch_x, hidden)\n",
        "        hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "\n",
        "    hidden = (torch.zeros([n_layers, 1, hidden_size], dtype=torch.float), torch.zeros([n_layers, 1, hidden_size], dtype=torch.float))\n",
        "    if epoch == 0 or (epoch+1) % print_interval == 0:\n",
        "        train_predict = model.forward(torch.tensor(train_input, dtype=torch.float), hidden)[0].detach().numpy()\n",
        "        pred_train += [list(train_predict.flatten())]\n",
        "        mape_train += [list(np.abs(train_predict - train_output).flatten()/train_output.flatten())]\n",
        "        \n",
        "        val_predict = model.forward(torch.tensor(val_input, dtype=torch.float), hidden)[0].detach().numpy()\n",
        "        pred_val += [list(val_predict.flatten())]\n",
        "        mape_val += [list(np.abs(val_predict - val_output).flatten()/val_output.flatten())]\n",
        "        \n",
        "        test_predict = model.forward(torch.tensor(test_input, dtype=torch.float), hidden)[0].detach().numpy()\n",
        "        pred_test += [list(test_predict.flatten())]\n",
        "        mape_test += [list(np.abs(test_predict - test_output).flatten()/test_output.flatten())]\n",
        "\n",
        "        MAPE_train = round(100*np.mean(mape_train[-1]),2)\n",
        "        MAPE_val   = round(100*np.mean(mape_val[-1]),2)\n",
        "        MAPE_test  = round(100*np.mean(mape_test[-1]),2)\n",
        "\n",
        "        print(\"epoch: {}\".format(epoch+1))\n",
        "        print(\"MAPE_train: {}%\".format(MAPE_train).ljust(25), end=\"\")\n",
        "        print(\"MAPE_val: {}%\".format(MAPE_val).ljust(25), end=\"\")\n",
        "        print(\"MAPE_test: {}%\".format(MAPE_test).ljust(25))\n",
        "        print(\"------------------------------------------------------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1\n",
            "MAPE_train: 1348.4%      MAPE_val: 1392.21%       MAPE_test: 1180.65%      \n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 10\n",
            "MAPE_train: 136.51%      MAPE_val: 146.28%        MAPE_test: 128.84%       \n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 20\n",
            "MAPE_train: 123.74%      MAPE_val: 130.66%        MAPE_test: 116.0%        \n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 30\n",
            "MAPE_train: 112.36%      MAPE_val: 118.28%        MAPE_test: 105.71%       \n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 40\n",
            "MAPE_train: 107.78%      MAPE_val: 113.21%        MAPE_test: 101.5%        \n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 50\n",
            "MAPE_train: 99.5%        MAPE_val: 104.49%        MAPE_test: 94.22%        \n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 60\n",
            "MAPE_train: 88.46%       MAPE_val: 92.98%         MAPE_test: 84.63%        \n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 70\n",
            "MAPE_train: 77.19%       MAPE_val: 81.23%         MAPE_test: 74.91%        \n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 80\n",
            "MAPE_train: 66.38%       MAPE_val: 70.1%          MAPE_test: 65.52%        \n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 90\n",
            "MAPE_train: 58.52%       MAPE_val: 62.08%         MAPE_test: 58.74%        \n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 100\n",
            "MAPE_train: 54.59%       MAPE_val: 58.09%         MAPE_test: 55.31%        \n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 110\n",
            "MAPE_train: 53.34%       MAPE_val: 56.83%         MAPE_test: 54.26%        \n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 120\n",
            "MAPE_train: 57.11%       MAPE_val: 60.62%         MAPE_test: 57.61%        \n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 130\n",
            "MAPE_train: 55.3%        MAPE_val: 58.88%         MAPE_test: 56.36%        \n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 140\n",
            "MAPE_train: 53.02%       MAPE_val: 56.6%          MAPE_test: 54.38%        \n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 150\n",
            "MAPE_train: 52.24%       MAPE_val: 55.92%         MAPE_test: 53.79%        \n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 160\n",
            "MAPE_train: 52.71%       MAPE_val: 56.49%         MAPE_test: 54.28%        \n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 170\n",
            "MAPE_train: 53.52%       MAPE_val: 57.38%         MAPE_test: 55.01%        \n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 180\n",
            "MAPE_train: 54.24%       MAPE_val: 58.17%         MAPE_test: 55.66%        \n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 190\n",
            "MAPE_train: 54.83%       MAPE_val: 58.8%          MAPE_test: 56.17%        \n",
            "------------------------------------------------------------------------------------------\n",
            "epoch: 200\n",
            "MAPE_train: 55.66%       MAPE_val: 59.62%         MAPE_test: 56.87%        \n",
            "------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_wer76M-YMb"
      },
      "source": [
        "# Produce results\n",
        "#\n",
        "select_num = np.argmin(np.mean(mape_val,axis=1))\n",
        "select_train = pd.DataFrame(np.array(pred_train[select_num][:]))\n",
        "select_val = pd.DataFrame(np.array(pred_val[select_num][:]))\n",
        "select_test = pd.DataFrame(np.array(pred_test[select_num][:]))\n",
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
