{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepComp_Comparison_SARSA_Wind.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "17dao2LY64aYIEp53suvEygPWgsO8auJq",
      "authorship_tag": "ABX9TyMnPKlZQ6DG1XY9mDRuOotm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbHJdT8wYOpD"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import collections\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4ulsTOX8Fr3"
      },
      "source": [
        "Renewable_Energy = \"Wind_Wallonie_Elia\"\r\n",
        "\r\n",
        "try:\r\n",
        "    address = \"/content/drive/My Drive/Doctor/Research/DRL/Colab/\"\r\n",
        "    data_train_csv1 = pd.read_csv(address+Renewable_Energy+'_16.csv', index_col=0)\r\n",
        "\r\n",
        "except:\r\n",
        "    address = \"\"\r\n",
        "    data_train_csv1 = pd.read_csv(address+Renewable_Energy+'_16.csv', index_col=0)\r\n",
        "\r\n",
        "data_train_csv2 = pd.read_csv(address+Renewable_Energy+'_17.csv', index_col=0)\r\n",
        "data_train_csv  = pd.concat([data_train_csv1, data_train_csv2])\r\n",
        "data_val_csv    = pd.read_csv(address+Renewable_Energy+'_18.csv', index_col=0)\r\n",
        "data_test_csv   = pd.read_csv(address+Renewable_Energy+'_19.csv', index_col=0)\r\n",
        "\r\n",
        "train_predict = np.array(pd.read_csv(address+\"results/\"+Renewable_Energy+\"_Model1_train.csv\", index_col=0))\r\n",
        "val_predict = np.array(pd.read_csv(address+\"results/\"+Renewable_Energy+\"_Model1_val.csv\", index_col=0))\r\n",
        "test_predict = np.array(pd.read_csv(address+\"results/\"+Renewable_Energy+\"_Model1_NEC.csv\", index_col=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxexWoP71odN"
      },
      "source": [
        "# Data Preprocessing\r\n",
        "\r\n",
        "Battery_Size = 0.5\r\n",
        "unit         = 4 #unit: 15 minute\r\n",
        "\r\n",
        "RE_Capacity1 = max(data_train_csv['Power(MW)'])\r\n",
        "RE_Capacity2 = max(data_val_csv['Power(MW)'])\r\n",
        "RE_Capacity3 = max(data_test_csv['Power(MW)'])\r\n",
        "\r\n",
        "size_train0 = int(len(data_train_csv)/unit)\r\n",
        "size_val0   = int(len(data_val_csv)/unit)\r\n",
        "size_test0  = int(len(data_test_csv)/unit)\r\n",
        "\r\n",
        "data_train0 = []\r\n",
        "data_train  = []\r\n",
        "for i in range(size_train0):\r\n",
        "    data_train0 += [round(pd.Series.mean(data_train_csv['Power(MW)'][i*unit:(i+1)*unit])/RE_Capacity1, 3)]\r\n",
        "    data_train  += [data_train0[i]] if data_train0[i] > 0 else []\r\n",
        "\r\n",
        "data_val0 = []\r\n",
        "data_val  = []\r\n",
        "for i in range(size_val0):\r\n",
        "    data_val0 += [round(pd.Series.mean(data_val_csv['Power(MW)'][i*unit:(i+1)*unit])/RE_Capacity2, 3)]\r\n",
        "    data_val  += [data_val0[i]] if data_val0[i] > 0 else []\r\n",
        "\r\n",
        "data_test0 = []\r\n",
        "data_test  = []\r\n",
        "for i in range(size_test0):\r\n",
        "    data_test0 += [round(pd.Series.mean(data_test_csv['Power(MW)'][i*unit:(i+1)*unit])/RE_Capacity3, 3)]\r\n",
        "    data_test  += [data_test0[i]] if data_test0[i] > 0 else []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGQrvrAZZ4SG",
        "outputId": "3c2847df-61b0-4a04-b6d5-f213f6e560cf"
      },
      "source": [
        "size_train = len(data_train)\n",
        "size_val = len(data_val)\n",
        "size_test = len(data_test)\n",
        "\n",
        "train_input = np.zeros((size_train-1, 1))\n",
        "train_output = np.zeros((size_train-1, 1))\n",
        "for i in range(size_train-1):\n",
        "    train_input[i,:] = data_train[i]\n",
        "    train_output[i,:] = data_train[i+1]\n",
        "\n",
        "val_input = np.zeros((size_val-1, 1))\n",
        "val_output = np.zeros((size_val-1, 1))\n",
        "for i in range(size_val-1):\n",
        "    val_input[i,:] = data_val[i]\n",
        "    val_output[i,:] = data_val[i+1]\n",
        "\n",
        "test_input = np.zeros((size_test-1, 1))\n",
        "test_output = np.zeros((size_test-1, 1))\n",
        "for i in range(size_test-1):\n",
        "    test_input[i,:] = data_test[i]\n",
        "    test_output[i,:] = data_test[i+1]\n",
        "\n",
        "print(\"MAPE_train: {}%\".format(round(100*np.mean(np.abs(train_predict - train_output)/train_output),2)).ljust(25), end=\"\")\n",
        "print(\"MAPE_val: {}%\".format(round(100*np.mean(np.abs(val_predict - val_output)/val_output),2)).ljust(25), end=\"\")\n",
        "print(\"MAPE_test: {}%\".format(round(100*np.mean(np.abs(test_predict - test_output)/test_output),2)).ljust(25))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MAPE_train: 50.91%       MAPE_val: 54.26%         MAPE_test: 52.1%         \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1orT-K3n34nr"
      },
      "source": [
        "# SARSA\n",
        "\n",
        "in_size       = 1\n",
        "out_size      = 5\n",
        "gamma         = 0.99\n",
        "epsilon       = 0.9\n",
        "batch_size    = 128\n",
        "learning_rate = 0.001\n",
        "\n",
        "class SARSA(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SARSA, self).__init__()\n",
        "        self.fc = nn.Linear(in_size, out_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "        \n",
        "def train_net(model, batch, optimizer):\n",
        "    s_lst, a_lst, r_lst, s_prime_lst = [], [], [], []\n",
        "\n",
        "    for transition in batch:\n",
        "        s, a, r, s_prime = transition\n",
        "        s_lst.append(s)\n",
        "        a_lst.append([a])\n",
        "        r_lst.append([r])\n",
        "        s_prime_lst.append(s_prime)\n",
        "\n",
        "    s,a,r,s_prime = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
        "                    torch.tensor(r_lst, dtype=torch.float), torch.tensor(s_prime_lst, dtype=torch.float)\n",
        "            \n",
        "    for i in range(10):\n",
        "        q_out = model.forward(s).gather(1,a)\n",
        "        target = r + gamma * torch.mean(model.forward(s_prime))\n",
        "        loss = F.mse_loss(q_out, target)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FM9xx8yDEf5x",
        "outputId": "3357d47c-6f08-429e-fb3e-c0ff311034dc"
      },
      "source": [
        "# Training SARSA\n",
        "\n",
        "E_max   = Battery_Size\n",
        "tdelta  = unit/4\n",
        "eff_c   = 0.9\n",
        "eff_d   = 0.9\n",
        "soc_min = 0.1\n",
        "soc_max = 0.9\n",
        "P_cmax  = Battery_Size/3\n",
        "P_dmax  = Battery_Size/3\n",
        "beta_c  = 0.01\n",
        "beta_d  = 0.01\n",
        "\n",
        "E_cmax = eff_c*P_cmax*tdelta\n",
        "E_dmax = (1/eff_d)*P_dmax*tdelta\n",
        "C_max  = int(out_size/2)\n",
        "\n",
        "total_episode = 20\n",
        "print_interval = 1\n",
        "\n",
        "model = SARSA()\n",
        "act_train,  act_val,  act_test  = [], [], [] # Controlled Value\n",
        "mape_train, mape_val, mape_test = [], [], [] # Mean Absolute Percentage Error\n",
        "ccr_train,  ccr_val,  ccr_test  = [], [], [] # Complete Compensation Ratio\n",
        "\n",
        "batch = collections.deque(maxlen=batch_size+1)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for n_epi in range(total_episode):\n",
        "    act_train  += [[]]; act_val  += [[]]; act_test  += [[]]\n",
        "    mape_train += [[]]; mape_val += [[]]; mape_test += [[]]\n",
        "    ccr_train  += [[]]; ccr_val  += [[]]; ccr_test  += [[]]\n",
        "\n",
        "    state = [E_max/2]\n",
        "    i = 0\n",
        "    while i < size_train-1:\n",
        "        for t in range(batch_size):\n",
        "            coin = torch.rand(1).item()\n",
        "            if coin < epsilon:\n",
        "                action = np.random.choice(range(out_size))\n",
        "            else:\n",
        "                Qout = model.forward(torch.tensor(state, dtype=torch.float))\n",
        "                action = np.argmax(Qout.tolist())\n",
        "            E_c = min(max((E_cmax/C_max)*(action-C_max), 0.0), E_max*soc_max - state[0])\n",
        "            E_d = min(max((E_dmax/C_max)*(C_max-action), 0.0), state[0] - E_max*soc_min)\n",
        "\n",
        "            real = train_output[i][0] + eff_d*E_d/tdelta - (1/eff_c)*E_c/tdelta\n",
        "            pred = train_predict[i][0]\n",
        "\n",
        "            E = state[0] + E_c - E_d\n",
        "            P_climit = min(P_cmax, (1/eff_c)*(E_max*soc_max - E)/tdelta)\n",
        "            P_dlimit = min(P_dmax, eff_d*(E - E_max*soc_min)/tdelta)\n",
        "            P_c = min(max(real-pred, 0), P_climit)\n",
        "            P_d = min(max(pred-real, 0), P_dlimit)\n",
        "            E_prime = E + eff_c*P_c*tdelta - (1/eff_d)*P_d*tdelta\n",
        "            disp = 0.0 if np.isclose(real - P_c + P_d, 0) else real - P_c + P_d\n",
        "            error = pred - disp\n",
        "            error_function = abs(error) + beta_c*P_c + beta_d*P_d\n",
        "\n",
        "            next_state = [E_prime]\n",
        "            reward = -error_function\n",
        "            batch.append((state, action, reward, next_state))\n",
        "            state = next_state[:]\n",
        "\n",
        "            act_train[n_epi]  += [E_c - E_d]\n",
        "            mape_train[n_epi]  += [abs((pred-disp)/disp)] if disp != 0 else [0]\n",
        "            ccr_train[n_epi] += [1 if np.isclose(pred-disp,0) else 0]\n",
        "            i += 1\n",
        "            if i == size_train-1:\n",
        "                break\n",
        "\n",
        "        if n_epi != 0:\n",
        "            train_net(model, batch, optimizer)\n",
        "    \n",
        "    state = [E_max/2]\n",
        "    for k in range(size_val-1):\n",
        "        Qout = model.forward(torch.tensor(state, dtype=torch.float))\n",
        "        action = np.argmax(Qout.tolist())\n",
        "        E_c = min(max((E_cmax/C_max)*(action-C_max), 0.0), E_max*soc_max - state[0])\n",
        "        E_d = min(max((E_dmax/C_max)*(C_max-action), 0.0), state[0] - E_max*soc_min)\n",
        "\n",
        "        real = val_output[k][0] + eff_d*E_d/tdelta - (1/eff_c)*E_c/tdelta\n",
        "        pred = val_predict[k][0]\n",
        "\n",
        "        E = state[0] + E_c - E_d\n",
        "        P_climit = min(P_cmax, (1/eff_c)*(E_max*soc_max - E)/tdelta)\n",
        "        P_dlimit = min(P_dmax, eff_d*(E - E_max*soc_min)/tdelta)\n",
        "        P_c = min(max(real-pred, 0), P_climit)\n",
        "        P_d = min(max(pred-real, 0), P_dlimit)\n",
        "        E_prime = E + eff_c*P_c*tdelta - (1/eff_d)*P_d*tdelta\n",
        "        disp = 0.0 if np.isclose(real - P_c + P_d, 0) else real - P_c + P_d\n",
        "        error = pred - disp\n",
        "        error_function = abs(error) + beta_c*P_c + beta_d*P_d\n",
        "\n",
        "        next_state = [E_prime]\n",
        "        state = next_state[:]\n",
        "\n",
        "        act_val[n_epi]  += [E_c - E_d]\n",
        "        mape_val[n_epi]  += [abs((pred-disp)/disp)] if disp != 0 else [0]\n",
        "        ccr_val[n_epi] += [1 if np.isclose(pred-disp,0) else 0]\n",
        "    \n",
        "    state = [E_max/2]\n",
        "    for l in range(size_test-1):\n",
        "        Qout = model.forward(torch.tensor(state, dtype=torch.float))\n",
        "        action = np.argmax(Qout.tolist())\n",
        "        E_c = min(max((E_cmax/C_max)*(action-C_max), 0.0), E_max*soc_max - state[0])\n",
        "        E_d = min(max((E_dmax/C_max)*(C_max-action), 0.0), state[0] - E_max*soc_min)\n",
        "\n",
        "        real = test_output[l][0] + eff_d*E_d/tdelta - (1/eff_c)*E_c/tdelta\n",
        "        pred = test_predict[l][0]\n",
        "\n",
        "        E = state[0] + E_c - E_d\n",
        "        P_climit = min(P_cmax, (1/eff_c)*(E_max*soc_max - E)/tdelta)\n",
        "        P_dlimit = min(P_dmax, eff_d*(E - E_max*soc_min)/tdelta)\n",
        "        P_c = min(max(real-pred, 0), P_climit)\n",
        "        P_d = min(max(pred-real, 0), P_dlimit)\n",
        "        E_prime = E + eff_c*P_c*tdelta - (1/eff_d)*P_d*tdelta\n",
        "        disp = 0.0 if np.isclose(real - P_c + P_d, 0) else real - P_c + P_d\n",
        "        error = pred - disp\n",
        "        error_function = abs(error) + beta_c*P_c + beta_d*P_d\n",
        "\n",
        "        next_state = [E_prime]\n",
        "        state = next_state[:]\n",
        "\n",
        "        act_test[n_epi]  += [E_c - E_d]\n",
        "        mape_test[n_epi]  += [abs((pred-disp)/disp)] if disp != 0 else [0]\n",
        "        ccr_test[n_epi] += [1 if np.isclose(pred-disp,0) else 0]\n",
        "    \n",
        "    if (n_epi+1)%print_interval == 0:\n",
        "        MAPE_train = round(100*np.mean(mape_train[n_epi]),2)\n",
        "        MAPE_val   = round(100*np.mean(mape_val[n_epi]),2)\n",
        "        MAPE_test  = round(100*np.mean(mape_test[n_epi]),2)\n",
        "        CCR_train  = round(np.mean(ccr_train[n_epi]),3)\n",
        "        CCR_val    = round(np.mean(ccr_val[n_epi]),3)\n",
        "        CCR_test   = round(np.mean(ccr_test[n_epi]),3)\n",
        "\n",
        "        print(\"episode: {}\".format(n_epi+1))\n",
        "        print(\"MAPE_train: {}%\".format(MAPE_train).ljust(25), end=\"\")\n",
        "        print(\"MAPE_val: {}%\".format(MAPE_val).ljust(25), end=\"\")\n",
        "        print(\"MAPE_test: {}%\".format(MAPE_test).ljust(25))\n",
        "        print(\"CCR_train: {}\".format(CCR_train).ljust(25), end=\"\")\n",
        "        print(\"CCR_val: {}\".format(CCR_val).ljust(25), end=\"\")\n",
        "        print(\"CCR_test: {}\".format(CCR_test).ljust(25))\n",
        "        print(\"------------------------------------------------------------------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode: 1\n",
            "MAPE_train: 66.91%       MAPE_val: 17.41%         MAPE_test: 18.03%        \n",
            "CCR_train: 0.581         CCR_val: 0.678           CCR_test: 0.678          \n",
            "------------------------------------------------------------------------------------------\n",
            "episode: 2\n",
            "MAPE_train: 61.94%       MAPE_val: 15.96%         MAPE_test: 16.26%        \n",
            "CCR_train: 0.586         CCR_val: 0.714           CCR_test: 0.708          \n",
            "------------------------------------------------------------------------------------------\n",
            "episode: 3\n",
            "MAPE_train: 60.17%       MAPE_val: 15.16%         MAPE_test: 15.79%        \n",
            "CCR_train: 0.584         CCR_val: 0.738           CCR_test: 0.737          \n",
            "------------------------------------------------------------------------------------------\n",
            "episode: 4\n",
            "MAPE_train: 65.61%       MAPE_val: 14.29%         MAPE_test: 14.59%        \n",
            "CCR_train: 0.581         CCR_val: 0.779           CCR_test: 0.778          \n",
            "------------------------------------------------------------------------------------------\n",
            "episode: 5\n",
            "MAPE_train: 59.18%       MAPE_val: 12.46%         MAPE_test: 12.99%        \n",
            "CCR_train: 0.586         CCR_val: 0.802           CCR_test: 0.799          \n",
            "------------------------------------------------------------------------------------------\n",
            "episode: 6\n",
            "MAPE_train: 85.73%       MAPE_val: 11.99%         MAPE_test: 12.47%        \n",
            "CCR_train: 0.587         CCR_val: 0.796           CCR_test: 0.784          \n",
            "------------------------------------------------------------------------------------------\n",
            "episode: 7\n",
            "MAPE_train: 63.24%       MAPE_val: 16.42%         MAPE_test: 17.08%        \n",
            "CCR_train: 0.589         CCR_val: 0.71            CCR_test: 0.702          \n",
            "------------------------------------------------------------------------------------------\n",
            "episode: 8\n",
            "MAPE_train: 60.89%       MAPE_val: 15.42%         MAPE_test: 15.65%        \n",
            "CCR_train: 0.584         CCR_val: 0.744           CCR_test: 0.738          \n",
            "------------------------------------------------------------------------------------------\n",
            "episode: 9\n",
            "MAPE_train: 62.27%       MAPE_val: 15.76%         MAPE_test: 16.26%        \n",
            "CCR_train: 0.583         CCR_val: 0.735           CCR_test: 0.725          \n",
            "------------------------------------------------------------------------------------------\n",
            "episode: 10\n",
            "MAPE_train: 107.82%      MAPE_val: 16.3%          MAPE_test: 16.13%        \n",
            "CCR_train: 0.586         CCR_val: 0.718           CCR_test: 0.71           \n",
            "------------------------------------------------------------------------------------------\n",
            "episode: 11\n",
            "MAPE_train: 70.86%       MAPE_val: 12.91%         MAPE_test: 12.52%        \n",
            "CCR_train: 0.585         CCR_val: 0.768           CCR_test: 0.754          \n",
            "------------------------------------------------------------------------------------------\n",
            "episode: 12\n",
            "MAPE_train: 64.22%       MAPE_val: 16.11%         MAPE_test: 16.69%        \n",
            "CCR_train: 0.588         CCR_val: 0.728           CCR_test: 0.713          \n",
            "------------------------------------------------------------------------------------------\n",
            "episode: 13\n",
            "MAPE_train: 62.3%        MAPE_val: 16.88%         MAPE_test: 17.45%        \n",
            "CCR_train: 0.587         CCR_val: 0.708           CCR_test: 0.709          \n",
            "------------------------------------------------------------------------------------------\n",
            "episode: 14\n",
            "MAPE_train: 127.16%      MAPE_val: 12.3%          MAPE_test: 12.64%        \n",
            "CCR_train: 0.587         CCR_val: 0.792           CCR_test: 0.782          \n",
            "------------------------------------------------------------------------------------------\n",
            "episode: 15\n",
            "MAPE_train: 60.84%       MAPE_val: 16.22%         MAPE_test: 16.55%        \n",
            "CCR_train: 0.588         CCR_val: 0.713           CCR_test: 0.708          \n",
            "------------------------------------------------------------------------------------------\n",
            "episode: 16\n",
            "MAPE_train: 95.54%       MAPE_val: 11.99%         MAPE_test: 12.46%        \n",
            "CCR_train: 0.587         CCR_val: 0.796           CCR_test: 0.784          \n",
            "------------------------------------------------------------------------------------------\n",
            "episode: 17\n",
            "MAPE_train: 74.39%       MAPE_val: 14.61%         MAPE_test: 14.89%        \n",
            "CCR_train: 0.589         CCR_val: 0.76            CCR_test: 0.75           \n",
            "------------------------------------------------------------------------------------------\n",
            "episode: 18\n",
            "MAPE_train: 71.31%       MAPE_val: 14.13%         MAPE_test: 14.06%        \n",
            "CCR_train: 0.584         CCR_val: 0.774           CCR_test: 0.767          \n",
            "------------------------------------------------------------------------------------------\n",
            "episode: 19\n",
            "MAPE_train: 56.95%       MAPE_val: 137.19%        MAPE_test: 129.13%       \n",
            "CCR_train: 0.584         CCR_val: 0.456           CCR_test: 0.474          \n",
            "------------------------------------------------------------------------------------------\n",
            "episode: 20\n",
            "MAPE_train: 71.67%       MAPE_val: 16.88%         MAPE_test: 17.45%        \n",
            "CCR_train: 0.585         CCR_val: 0.708           CCR_test: 0.709          \n",
            "------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxA-zDA6Y02K"
      },
      "source": [
        "# Produce results\r\n",
        "#\r\n",
        "#select_num = np.argmin(np.mean(mape_val,axis=1))\r\n",
        "#select = pd.DataFrame(np.array(act_test[select_num][:]))\r\n",
        "#select.to_csv(address+Renewable_Energy+\"_Model2_ECC+_\"+str(int(100*E_max))+\".csv\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}