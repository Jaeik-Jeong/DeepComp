{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOR7/LU+YumMYKOSCXAjbUA"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"RrPmQdPSvSE_"},"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HsI_WpHF2n0-"},"source":["Renewable_Energy = \"Solar_PBE\" # Solar_PBE / Wind_Wallonie_Elia\n","address = \"https://raw.githubusercontent.com/Jaeik-Jeong/DeepComp/main/data/\"\n","\n","data_train_csv1 = pd.read_csv(address+Renewable_Energy+'_16.csv', index_col=0)\n","data_train_csv2 = pd.read_csv(address+Renewable_Energy+'_17.csv', index_col=0)\n","data_train_csv  = pd.concat([data_train_csv1, data_train_csv2])\n","data_val_csv    = pd.read_csv(address+Renewable_Energy+'_18.csv', index_col=0)\n","data_test_csv   = pd.read_csv(address+Renewable_Energy+'_19.csv', index_col=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kl1lRaIyvSiP"},"source":["# Data Preprocessing\n","\n","unit = 4 #unit: 15 minute\n","\n","RE_Capacity1 = max(data_train_csv['Power(MW)'])\n","RE_Capacity2 = max(data_val_csv['Power(MW)'])\n","RE_Capacity3 = max(data_test_csv['Power(MW)'])\n","\n","size_train0 = int(len(data_train_csv)/unit)\n","size_val0   = int(len(data_val_csv)/unit)\n","size_test0  = int(len(data_test_csv)/unit)\n","\n","data_train0 = []\n","data_train  = []\n","for i in range(size_train0):\n","    data_train0 += [round(pd.Series.mean(data_train_csv['Power(MW)'][i*unit:(i+1)*unit])/RE_Capacity1, 3)]\n","    data_train  += [data_train0[i]] if data_train0[i] > 0 else []\n","\n","data_val0 = []\n","data_val  = []\n","for i in range(size_val0):\n","    data_val0 += [round(pd.Series.mean(data_val_csv['Power(MW)'][i*unit:(i+1)*unit])/RE_Capacity2, 3)]\n","    data_val  += [data_val0[i]] if data_val0[i] > 0 else []\n","\n","data_test0 = []\n","data_test  = []\n","for i in range(size_test0):\n","    data_test0 += [round(pd.Series.mean(data_test_csv['Power(MW)'][i*unit:(i+1)*unit])/RE_Capacity3, 3)]\n","    data_test  += [data_test0[i]] if data_test0[i] > 0 else []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R_Kq8vMEvTk9"},"source":["# Persistence Model (PM)\n","\n","PF_pred_test = []\n","for i in range(len(data_test)-1):\n","    PF_pred_test += [data_test[i]]\n","\n","test_output = np.array(data_test[1:])\n","test_predict = np.array(PF_pred_test)\n","MAPE_test = np.mean(np.abs(test_predict - test_output).flatten()/test_output.flatten())\n","print(\"MAPE_test: {}%\".format(round(100*MAPE_test,2)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FbalbM1-wSDu"},"source":["# AutoRegressive (AR)\n","\n","past = 5\n","\n","size_train = len(data_train)\n","size_val   = len(data_val)\n","size_test  = len(data_test)\n","\n","train_input = np.zeros((size_train-past, past))\n","train_output = np.zeros((size_train-past, 1))\n","for i in range(size_train-past):\n","    train_input[i,:] = np.reshape(data_train[i:i+past], (past))\n","    train_output[i,:] = data_train[i+past]\n","\n","val_input = np.zeros((size_val-past, past))\n","val_output = np.zeros((size_val-past, 1))\n","for i in range(size_val-past):\n","    val_input[i,:] = np.reshape(data_val[i:i+past], (past))\n","    val_output[i,:] = data_val[i+past]\n","\n","test_input = np.zeros((size_test-past, past))\n","test_output = np.zeros((size_test-past, 1))\n","for i in range(size_test-past):\n","    test_input[i,:] = np.reshape(data_test[i:i+past], (past))\n","    test_output[i,:] = data_test[i+past]\n","\n","weight = np.matmul(np.linalg.pinv(train_input), train_output)\n","val_predict  = np.matmul(val_input, weight)\n","test_predict = np.matmul(test_input, weight)\n","MAPE_val  = np.mean(np.abs(val_predict - val_output).flatten()/val_output.flatten())\n","MAPE_test = np.mean(np.abs(test_predict - test_output).flatten()/test_output.flatten())\n","print(\"MAPE_val: {}%, MAPE_test: {}%\".format(round(100*MAPE_val,2),round(100*MAPE_test,2)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IYRc9xWVi17b"},"source":["# Feedforward Neural Network (FNN)\n","\n","learning_rate = 1e-3\n","in_size       = past\n","hidden_size   = 16\n","out_size      = 1\n","\n","class NN(nn.Module):\n","    def __init__(self, learning_rate, in_size, hidden_size, out_size):\n","        super(NN, self).__init__()\n","        self.learning_rate = learning_rate\n","        self.in_size       = in_size\n","        self.hidden_size   = hidden_size\n","        self.out_size      = out_size\n","        \n","        self.layer1 = nn.Linear(self.in_size, self.hidden_size)\n","        self.layer2 = nn.Linear(self.hidden_size,self.hidden_size)\n","        self.fc_out = nn.Linear(self.hidden_size, self.out_size)\n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n","\n","    def forward(self, x):\n","        x = F.relu(self.layer1(x))\n","        x = F.relu(self.layer2(x))\n","        out = self.fc_out(x)\n","        return out\n","        \n","    def train_net(self, x, y):\n","        x, y = torch.tensor(x,dtype=torch.float), torch.tensor(y,dtype=torch.float)\n","        loss = F.mse_loss(self.forward(x), y)\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","model = NN(learning_rate, in_size, hidden_size, out_size)\n","batch_size = 128\n","total_batch = int((size_train-past)/batch_size) + 1\n","total_epoch    = 500\n","print_interval = 10\n","\n","for epoch in range(total_epoch):\n","    for i in range(total_batch):\n","        batch_x = train_input[batch_size*i:batch_size*(i+1),:]\n","        batch_y = train_output[batch_size*i:batch_size*(i+1),:]\n","        model.train_net(batch_x, batch_y)\n","\n","    if epoch == 0 or (epoch+1) % print_interval == 0:\n","        val_predict = model.forward(torch.tensor(val_input, dtype=torch.float)).detach().numpy()\n","        test_predict = model.forward(torch.tensor(test_input, dtype=torch.float)).detach().numpy()\n","        MAPE_val  = np.mean(np.abs(val_predict - val_output).flatten()/val_output.flatten())\n","        MAPE_test = np.mean(np.abs(test_predict - test_output).flatten()/test_output.flatten())\n","        print(\"epoch: {}, MAPE_val: {}%, MAPE_test: {}%\".format(epoch+1, round(100*MAPE_val,2),round(100*MAPE_test,2)))"],"execution_count":null,"outputs":[]}]}